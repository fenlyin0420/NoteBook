# 预训练数据集（Pre-training Dataset）​

#### **定义**
用于无监督训练语言模型（如GPT、BERT）的大规模文本集合，模型通过预测文本片段（如掩码词、下一句）学习**通用语言模式**。
#### ​**特点**
- ​**数据规模**：海量（TB级），覆盖网页、书籍、论文、代码等多领域文本。
    - 示例：GPT-3的预训练数据包含约45TB的文本。
- ​**无监督性**：无需人工标注，直接利用原始文本的结构（如上下文关系）进行自监督学习。
- ​**多样性**：涵盖广泛主题和语言风格，使模型具备跨任务的基础能力。


# SFT 数据集
**SFT数据集**​（Supervised Fine-Tuning Dataset）是用于对预训练语言模型（如GPT、BERT等）进行**监督式微调（Supervised Fine-Tuning）​**的标注数据。
### **核心概念**
1. ​**监督微调（SFT）​**：

    - 在预训练模型（通过无监督学习从海量文本中学习通用语言模式）的基础上，使用**标注数据**对模型进行有监督的微调，使其适应具体任务（如对话生成、文本摘要、指令遵循等）。
2. ​**数据集特点**：
    - 包含**输入-输出对**​（input-output pairs），例如：
        - ​**对话场景**：用户输入（Prompt） + 期望的模型回答（Response）。
        - ​**指令场景**：任务指令（Instruction） + 正确执行结果（Output）。
    - 数据通常由人工标注或经过筛选/清洗的高质量文本组成。

### **典型应用场景**
- ​**对话模型**​（如ChatGPT）：
    - 数据示例：`{"prompt": "如何泡茶？", "response": "1. 烧水... 2. 放入茶叶..."}`
- ​**指令遵循模型**​（如InstructGPT）：
    - 数据示例：`{"instruction": "写一首关于春天的诗", "output": "春风拂面花香溢..."}`

### **SFT数据集 vs 其他数据**
1. ​**预训练数据**：
    - 无监督，海量无标注文本（如书籍、网页），用于学习语言的基础模式。
2. ​**SFT数据**：
    - 有监督，少量但高质量的标注数据，用于调整模型生成特定类型的输出。
3. ​**RLHF数据**​（基于人类反馈的强化学习）：
    - 用于进一步优化模型，通常包含人类对生成结果的偏好排序（如哪个回答更好）。

### ​**重要性**
- ​**质量决定性能**：SFT数据的多样性和准确性直接影响模型在目标任务上的表现。
- ​**缓解模型缺陷**：通过标注数据修正预训练模型的错误倾向（如生成有害内容或不相关回答）。
- ​**任务适配**：使通用模型具备执行具体任务的能力（如编程助手、客服机器人）。

# RLHF数据集
Reinforcement Learning from Human Feedback Dataset
#### **定义**
用于通过强化学习优化模型的标注数据，包含人类对模型输出质量的**偏好排序**或**评分**，使模型生成更符合人类价值观的结果。
#### ​**特点**
- ​**数据形式**：
    - ​**排序对**：针对同一输入（Prompt），标注员对多个模型输出的质量进行排序。  
        示例：`Prompt: "解释量子力学" → Response A > Response B > Response C`
    - ​**评分数据**：直接对单个输出打分（如1-5分）。
- ​**标注目标**：
    - 对齐人类偏好（如安全性、有用性、无害性）。
    - 修正模型缺陷（如胡编乱造、偏见输出）。

# 模型训练流程

1. ​**预训练**：用海量网页/书籍数据训练GPT基座模型。
2. ​**SFT微调**：用人工编写的对话数据（如`用户问→助手答`）调整模型为对话形态。
3. ​**RLHF优化**：通过人类对回答的排序数据训练奖励模型，再用强化学习进一步微调。
通过三者的结合，模型最终实现：​**通用语言能力（预训练）→ 任务专用能力（SFT）→ 安全可控性（RLHF）​**的递进优化。